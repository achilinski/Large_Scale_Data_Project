{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025acb04-7299-4546-a46c-79e7924ce6d2",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "\n",
    "\n",
    "# Finds out the index of \"name\" in the array firstLine \n",
    "# returns -1 if it cannot find it\n",
    "def findCol(firstLine, name):\n",
    "\tif name in firstLine:\n",
    "\t\treturn firstLine.index(name)\n",
    "\telse:\n",
    "\t\treturn -1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "044d1b0c-cc3f-437d-b790-87d3844f6962",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[1]) created by __init__ at /tmp/ipykernel_9753/2735925061.py:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#### Driver program\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# start spark with 1 worker thread\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal[1]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m sc\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m sqlContext \u001b[38;5;241m=\u001b[39m SQLContext(sc)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    454\u001b[0m             currentAppName,\n\u001b[1;32m    455\u001b[0m             currentMaster,\n\u001b[1;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[1;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[1;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[1;32m    459\u001b[0m         )\n\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[1]) created by __init__ at /tmp/ipykernel_9753/2735925061.py:4 "
     ]
    }
   ],
   "source": [
    "\n",
    "#### Driver program\n",
    "\n",
    "# start spark with 1 worker thread\n",
    "sc = SparkContext(\"local[1]\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c1f1a1842de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task Events: evicted und killed getrennt\n",
    "\n",
    "input_paths = [\n",
    "    \"google-dataset/task_events/part-00265-of-00500.csv.gz\",\n",
    "    \"google-dataset/task_events/part-00266-of-00500.csv.gz\",\n",
    "    \"google-dataset/task_events/part-00267-of-00500.csv.gz\",\n",
    "    \"google-dataset/task_events/part-00268-of-00500.csv.gz\",\n",
    "    \"google-dataset/task_events/part-00269-of-00500.csv.gz\",\n",
    "]\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# no header so header=False\n",
    "df = sqlContext.read.csv(input_paths, header=False, inferSchema=True)\n",
    "\n",
    "df = df.withColumnRenamed(\"_c2\", \"job_id\") \\\n",
    "       .withColumnRenamed(\"_c3\", \"task_id\") \\\n",
    "       .withColumnRenamed(\"_c5\", \"event_type\")\n",
    "\n",
    "total_tasks = df.select(\"job_id\", \"task_id\").distinct().count()\n",
    "\n",
    "# evicted = 2, killed = 5\n",
    "evicted_tasks = (\n",
    "    df.filter(df.event_type == 2)\n",
    "      .select(\"job_id\", \"task_id\")\n",
    "      .distinct()\n",
    "      .count()\n",
    ")\n",
    "\n",
    "killed_tasks = (\n",
    "    df.filter(df.event_type == 5)\n",
    "      .select(\"job_id\", \"task_id\")\n",
    "      .distinct()\n",
    "      .count()\n",
    ")\n",
    "\n",
    "combined_tasks = (\n",
    "    df.filter(df.event_type.isin([2,5]))\n",
    "      .select(\"job_id\", \"task_id\")\n",
    "      .distinct()\n",
    "      .count()\n",
    ")\n",
    "\n",
    "def pct(part, whole):\n",
    "    return (part / whole * 100.0) if whole > 0 else 0.0\n",
    "\n",
    "print(\"=== Task Events Summary ===\")\n",
    "print(\"Total number of tasks: \", total_tasks)\n",
    "print(\"Evicted tasks (code=2): {} ({:.2f}%)\".format(evicted_tasks, pct(evicted_tasks, total_tasks)))\n",
    "print(\"Killed tasks   (code=5): {} ({:.2f}%)\".format(killed_tasks, pct(killed_tasks, total_tasks)))\n",
    "print(\"Evicted or Killed (combined): {} ({:.2f}%)\".format(combined_tasks, pct(combined_tasks, total_tasks)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bd5ed699a71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Job events: evicted und killed getrennt\n",
    "\n",
    "input_paths = [\n",
    "    \"google-dataset/job_events/part-00265-of-00500.csv.gz\",\n",
    "    \"google-dataset/job_events/part-00266-of-00500.csv.gz\",\n",
    "    \"google-dataset/job_events/part-00267-of-00500.csv.gz\",\n",
    "    \"google-dataset/job_events/part-00268-of-00500.csv.gz\",\n",
    "    \"google-dataset/job_events/part-00269-of-00500.csv.gz\",\n",
    "]\n",
    "\n",
    "# no header\n",
    "df = sqlContext.read.csv(input_paths, header=False, inferSchema=True)\n",
    "\n",
    "df = df.withColumnRenamed(\"_c2\", \"job_id\") \\\n",
    "       .withColumnRenamed(\"_c3\", \"event_type\")\n",
    "\n",
    "total_jobs = df.select(\"job_id\").distinct().count()\n",
    "\n",
    "evicted_jobs = (\n",
    "    df.filter(df.event_type == 2)\n",
    "      .select(\"job_id\")\n",
    "      .distinct()\n",
    "      .count()\n",
    ")\n",
    "\n",
    "killed_jobs = (\n",
    "    df.filter(df.event_type == 5)\n",
    "      .select(\"job_id\")\n",
    "      .distinct()\n",
    "      .count()\n",
    ")\n",
    "\n",
    "combined_jobs = (\n",
    "    df.filter(df.event_type.isin([2,5]))\n",
    "      .select(\"job_id\")\n",
    "      .distinct()\n",
    "      .count()\n",
    ")\n",
    "\n",
    "print(\"=== Job Events Summary ===\")\n",
    "print(\"Total number of jobs: \", total_jobs)\n",
    "print(\"Evicted jobs (code=2): {} ({:.2f}%)\".format(evicted_jobs, pct(evicted_jobs, total_jobs)))\n",
    "print(\"Killed jobs   (code=5): {} ({:.2f}%)\".format(killed_jobs, pct(killed_jobs, total_jobs)))\n",
    "print(\"Evicted or Killed (combined): {} ({:.2f}%)\".format(combined_jobs, pct(combined_jobs, total_jobs)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
