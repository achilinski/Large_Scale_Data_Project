{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "025acb04-7299-4546-a46c-79e7924ce6d2",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#### Driver program\n",
    "\n",
    "import sys\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = SparkContext(\"local[8]\")\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sqlContext = SQLContext(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754c1f1a1842de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution:\n",
      "+--------------------+--------+--------------------+\n",
      "|num_machines_for_job|num_jobs|    fraction_of_jobs|\n",
      "+--------------------+--------+--------------------+\n",
      "|                   1|    4267|  0.6703849175176748|\n",
      "|                   2|     532| 0.08358208955223881|\n",
      "|                   3|     121|  0.0190102120974077|\n",
      "|                   4|      93|0.014611154752553025|\n",
      "|                   5|      35|0.005498821681068...|\n",
      "|                   6|      44|0.006912804399057345|\n",
      "|                   7|      23|0.003613511390416...|\n",
      "|                   8|      15|0.002356637863315004|\n",
      "|                   9|      47|0.007384131971720346|\n",
      "|                  10|      19|0.002985074626865...|\n",
      "|                  11|      50|0.007855459544383346|\n",
      "|                  12|      34|0.005341712490180676|\n",
      "|                  13|       8|0.001256873527101...|\n",
      "|                  14|      35|0.005498821681068...|\n",
      "|                  15|     240|0.037706205813040065|\n",
      "|                  16|      31|0.004870384917517675|\n",
      "|                  17|      11|0.001728201099764...|\n",
      "|                  18|      19|0.002985074626865...|\n",
      "|                  19|       8|0.001256873527101...|\n",
      "|                  20|      28|0.004399057344854674|\n",
      "+--------------------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Group by job id and count machine IDs to get different machines\n",
    "\n",
    "input_paths = [\n",
    "    \"google-dataset/task_events/part-00265-of-00500.csv.gz\",\n",
    "    \"google-dataset/task_events/part-00266-of-00500.csv.gz\",\n",
    "    \"google-dataset/task_events/part-00267-of-00500.csv.gz\",\n",
    "    \"google-dataset/task_events/part-00268-of-00500.csv.gz\",\n",
    "    \"google-dataset/task_events/part-00269-of-00500.csv.gz\",\n",
    "]\n",
    "\n",
    "# task_events schema.csv:\n",
    "# _c2 = job ID (field 3)\n",
    "# _c3 = task index (field 4)\n",
    "# _c4 = machine ID (field 5)\n",
    "df = sqlContext.read.csv(input_paths, header=False, inferSchema=True) \\\n",
    "    .withColumnRenamed(\"_c2\", \"job_id\") \\\n",
    "    .withColumnRenamed(\"_c3\", \"task_id\") \\\n",
    "    .withColumnRenamed(\"_c4\", \"machine_id\")\n",
    "\n",
    "# for each (job_id, task_id) collect machines (in case of migration during on task)\n",
    "# in general collecting for job_id should be sufficiant\n",
    "job_task_machines = (\n",
    "    df.select(\"job_id\", \"task_id\", \"machine_id\")\n",
    "      .dropna(subset=[\"machine_id\"])\n",
    "      .groupBy(\"job_id\", \"task_id\")\n",
    "      .agg(F.collect_set(\"machine_id\").alias(\"machines_for_task\"))\n",
    ")\n",
    "\n",
    "# count different machines\n",
    "jobs_machine_stats = (\n",
    "    job_task_machines\n",
    "    .select(\n",
    "        \"job_id\",\n",
    "        F.explode(\"machines_for_task\").alias(\"machine_id\")\n",
    "    )\n",
    "    .groupBy(\"job_id\")\n",
    "    .agg(\n",
    "        F.countDistinct(\"machine_id\").alias(\"num_machines_for_job\")\n",
    "    )\n",
    "    .orderBy(F.col(\"num_machines_for_job\").desc())\n",
    ")\n",
    "\n",
    "# print for debug purposes\n",
    "#print(\"Jobs with most different machines:\")\n",
    "#jobs_machine_stats.show(20, truncate=False)\n",
    "#print(\"jobs on one machine:\")\n",
    "#jobs_machine_stats.filter(F.col(\"num_machines_for_job\") == 1).show(20, truncate=False)\n",
    "\n",
    "\n",
    "total_jobs = jobs_machine_stats.count()\n",
    "\n",
    "distribution = (\n",
    "    jobs_machine_stats\n",
    "    .groupBy(\"num_machines_for_job\")\n",
    "    .agg(F.count(\"*\").alias(\"num_jobs\"))\n",
    "    .withColumn(\n",
    "        \"fraction_of_jobs\",\n",
    "        F.col(\"num_jobs\") / F.lit(total_jobs)\n",
    "    )\n",
    "    .orderBy(\"num_machines_for_job\")\n",
    ")\n",
    "\n",
    "print(\"Distribution:\")\n",
    "distribution.show(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bd5ed699a71e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
